{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FinAminToastCrunch/Probabalistic_Graph_Residual/blob/main/notebook_version_row_wise_kl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63b7jO97FcE-",
        "outputId": "0c1aec15-38bd-4f7d-c7d0-5137966ee6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n",
            "\u001b[K     |████████████████████████████████| 348 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting alembic>=1.5.0\n",
            "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
            "\u001b[K     |████████████████████████████████| 209 kB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (4.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.3)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.43)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (6.0)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.2.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna) (5.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 60.0 MB/s \n",
            "\u001b[?25hCollecting autopage>=0.4.0\n",
            "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (22.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=72d90ec22a4f4cf15b98267c8a836eed61197c0cb75bb52876a5607a992b4123\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.2.3 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 optuna-3.0.3 pbr-5.11.0 pyperclip-1.8.2 stevedore-3.5.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.12.0%2Bcu113/torch_sparse-0.6.15-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.15\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.1.0.post1.tar.gz (467 kB)\n",
            "\u001b[K     |████████████████████████████████| 467 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.1.0.post1-py3-none-any.whl size=689859 sha256=03122bf36e00c21d07b77ee7534ae8adfd578f4f142b166031944977bd1ea7a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/cb/43/f7f2e472de4d7cff31bceddadc36d634e1e545fbc17961c282\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.1.0.post1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deeprobust\n",
            "  Downloading deeprobust-0.2.5-py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.13.1+cu113)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (2.6.3)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.17.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.21.6)\n",
            "Collecting tensorboardX>=2.0\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (0.18.3)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (3.2.2)\n",
            "Collecting texttable>=1.6.2\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm>=3.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (4.64.1)\n",
            "Collecting gensim<4.0,>=3.8\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from deeprobust) (7.1.2)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.8->deeprobust) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.8->deeprobust) (5.2.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.1->deeprobust) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1.1->deeprobust) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->deeprobust) (57.4.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.0->deeprobust) (1.3.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->deeprobust) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->deeprobust) (1.2.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=2.0->deeprobust) (3.19.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deeprobust) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.48.0->deeprobust) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.4.0->deeprobust) (3.0.4)\n",
            "Installing collected packages: texttable, tensorboardX, gensim, deeprobust\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed deeprobust-0.2.5 gensim-3.8.3 tensorboardX-2.5.1 texttable-1.6.4\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.1+cu113.html\n",
        "!pip install torch-geometric\n",
        "!pip install deeprobust"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UH5uPkcNF-Jx"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  dataset = 'Cora'\n",
        "  seed = 15\n",
        "  model = 'AirGNN'\n",
        "  alpha = 0.1\n",
        "  lambda_amp = 0.1\n",
        "  lcc = False\n",
        "  normalize_features = True\n",
        "  random_splits = False\n",
        "  runs = 10\n",
        "  epochs = 10\n",
        "  lr = 0.01\n",
        "  weight_decay = 0.0005\n",
        "  early_stopping = 100\n",
        "  hidden = 64\n",
        "  dropout = 0.8\n",
        "  K = 10\n",
        "  model_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kBsHhz7FQxY",
        "outputId": "ca63751f-4e64-4179-c8d2-b9b6d4bc7d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "#adv_attack.py\n",
        "\n",
        "from deeprobust.graph.data import Dataset\n",
        "from deeprobust.graph.defense import GCN\n",
        "from deeprobust.graph.targeted_attack import Nettack\n",
        "import pickle\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "#from datasets import uniqueId, str2bool\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device:\", device)\n",
        "\n",
        "def cache_nettack_feature_attack_data(args):\n",
        "    n_perturbations_candidates = [0, 1, 2, 5, 10, 20, 50, 80]\n",
        "    print(\"====preparing dataset: %s=====\" % (args.dataset))\n",
        "    cache_dataset(args.dataset, n_perturbations_candidates, args)\n",
        "\n",
        "\n",
        "def cache_dataset(dataset_name, n_perturbations_candidates, args):\n",
        "    data = Dataset(root='/tmp/', name=(dataset_name).lower(), seed=args.seed)\n",
        "    adj, features, labels = data.adj, data.features, data.labels\n",
        "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "    surrogate = GCN(nfeat=features.shape[1], nclass=labels.max().item() + 1,\n",
        "                    nhid=16, dropout=0, with_relu=False, with_bias=False, device=device)\n",
        "    surrogate = surrogate.to(device)\n",
        "    surrogate.fit(features, adj, labels, idx_train)\n",
        "\n",
        "    node_list = select_nodes(data)\n",
        "    print(data, node_list, args)\n",
        "\n",
        "    for target_node in tqdm(node_list):\n",
        "        for perturbation in n_perturbations_candidates:\n",
        "\n",
        "            uid = uniqueId(dataset_name, target_node, perturbation)\n",
        "            n_perturbations = int(perturbation)\n",
        "            if n_perturbations != 0:\n",
        "                model = Nettack(surrogate, nnodes=adj.shape[0], attack_structure=False,\n",
        "                                attack_features=True, device=device)\n",
        "                model = model.to(device)\n",
        "                model.attack(features, adj, labels, target_node, n_perturbations, verbose=False)\n",
        "                modified_features = model.modified_features\n",
        "                data.features = modified_features\n",
        "\n",
        "            pickle.dump(data, open(\"./fixed_data/adv_attack/\" + uid + \".pickle\", 'wb'))\n",
        "            print(uid, \"has been save\")\n",
        "\n",
        "def classification_margin(output, true_label):\n",
        "    \"\"\"Calculate classification margin for outputs.\n",
        "    `probs_true_label - probs_best_second_class`\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.Tensor\n",
        "        output vector (1 dimension)\n",
        "    true_label: int\n",
        "        true label for this node\n",
        "    Returns\n",
        "    -------\n",
        "    list\n",
        "        classification margin for this node\n",
        "    \"\"\"\n",
        "    probs = torch.exp(output)\n",
        "    probs_true_label = probs[true_label].clone()\n",
        "    probs[true_label] = 0\n",
        "    probs_best_second_class = probs[probs.argmax()]\n",
        "    return (probs_true_label - probs_best_second_class).item()\n",
        "\n",
        "\n",
        "def select_nodes(data, target_gcn=None):\n",
        "    '''\n",
        "    selecting nodes as reported in nettack paper:\n",
        "    (i) the 10 nodes with highest margin of classification, i.e. they are clearly correctly classified,\n",
        "    (ii) the 10 nodes with lowest margin (but still correctly classified) and\n",
        "    (iii) 20 more nodes randomly\n",
        "    '''\n",
        "    adj, features, labels = data.adj, data.features, data.labels\n",
        "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
        "\n",
        "    if target_gcn is None:\n",
        "        target_gcn = GCN(nfeat=features.shape[1],\n",
        "                         nhid=16,\n",
        "                         nclass=labels.max().item() + 1,\n",
        "                         dropout=0.5, device=device)\n",
        "        target_gcn = target_gcn.to(device)\n",
        "        target_gcn.fit(features, adj, labels, idx_train, idx_val, patience=30)\n",
        "    target_gcn.eval()\n",
        "    output = target_gcn.predict()\n",
        "\n",
        "    margin_dict = {}\n",
        "    for idx in idx_test:\n",
        "        margin = classification_margin(output[idx], labels[idx])\n",
        "        if margin < 0:  # only keep the nodes correctly classified\n",
        "            continue\n",
        "        margin_dict[idx] = margin\n",
        "    sorted_margins = sorted(margin_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    high = [x for x, y in sorted_margins[: 10]]\n",
        "    low = [x for x, y in sorted_margins[-10:]]\n",
        "    other = [x for x, y in sorted_margins[10: -10]]\n",
        "    other = np.random.choice(other, 20, replace=False).tolist()\n",
        "\n",
        "    return high + low + other\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     # parser = argparse.ArgumentParser()\n",
        "#     # parser.add_argument('--dataset', type=str, required=True)\n",
        "#     # parser.add_argument('--seed', type=int, default=15)\n",
        "#     # args = parser.parse_args()\n",
        "#     args = Args()\n",
        "#     print('arg : ', args)\n",
        "#     cache_nettack_feature_attack_data(args)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SjTLKod8FZIX"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "\n",
        "import os.path as osp\n",
        "import torch\n",
        "\n",
        "from torch_geometric.datasets import Planetoid, Coauthor, Amazon\n",
        "import torch_geometric.transforms as T\n",
        "from deeprobust.graph.data import Dataset, Dpr2Pyg\n",
        "\n",
        "import argparse\n",
        "\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Unsupported value encountered.')\n",
        "\n",
        "def uniqueId(dataset_name, target_node, perturbation):\n",
        "    return dataset_name.lower() +\"_\"+str(target_node)+\"_\"+str(perturbation)\n",
        "\n",
        "\n",
        "def prepare_data(args, lcc=False):\n",
        "    transform = T.ToSparseTensor()\n",
        "    if args.dataset == \"Cora\" or args.dataset == \"CiteSeer\" or args.dataset == \"PubMed\":\n",
        "        if lcc:\n",
        "            dpr_data = Dataset(root='/tmp/', name=(args.dataset).lower())\n",
        "            dataset = Dpr2Pyg(dpr_data, transform=transform)\n",
        "        else:\n",
        "            dataset = get_planetoid_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_planetoid_splits if args.random_splits else None\n",
        "\n",
        "    elif args.dataset == \"cs\" or args.dataset == \"physics\":\n",
        "        dataset = get_coauthor_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_coauthor_amazon_splits\n",
        "\n",
        "    elif args.dataset == \"computers\" or args.dataset == \"photo\":\n",
        "        dataset = get_amazon_dataset(args.dataset, args.normalize_features, transform)\n",
        "        permute_masks = random_coauthor_amazon_splits\n",
        "    print(\"Data:\", dataset[0])\n",
        "\n",
        "    return dataset, permute_masks\n",
        "\n",
        "\n",
        "def get_planetoid_dataset(name, normalize_features=False, transform=None):\n",
        "    #path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    #print(f\"path = {path}\")\n",
        "    path = '/content/../data/Cora'\n",
        "    dataset = Planetoid(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_coauthor_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    dataset = Coauthor(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_amazon_dataset(name, normalize_features=False, transform=None):\n",
        "    path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', name)\n",
        "    dataset = Amazon(path, name)\n",
        "\n",
        "    if transform is not None and normalize_features:\n",
        "        dataset.transform = T.Compose([T.NormalizeFeatures(), transform])\n",
        "    elif normalize_features:\n",
        "        dataset.transform = T.NormalizeFeatures()\n",
        "    elif transform is not None:\n",
        "        dataset.transform = transform\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def index_to_mask(index, size):\n",
        "    mask = torch.zeros(size, dtype=torch.bool, device=index.device)\n",
        "    mask[index] = 1\n",
        "    return mask\n",
        "\n",
        "\n",
        "def random_planetoid_splits(data, num_classes, lcc_mask=None):\n",
        "    # Set new random planetoid splits:\n",
        "    # * 20 * num_classes labels for training\n",
        "    # * 500 labels for validation\n",
        "    # * 1000 labels for testing\n",
        "\n",
        "    indices = []\n",
        "    if lcc_mask is not None:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y[lcc_mask] == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0))]\n",
        "            indices.append(index)\n",
        "    else:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0))]\n",
        "            indices.append(index)\n",
        "\n",
        "    train_index = torch.cat([i[:20] for i in indices], dim=0)\n",
        "\n",
        "    rest_index = torch.cat([i[20:] for i in indices], dim=0)\n",
        "    rest_index = rest_index[torch.randperm(rest_index.size(0))]\n",
        "\n",
        "    data.train_mask = index_to_mask(train_index, size=data.num_nodes)\n",
        "    data.val_mask = index_to_mask(rest_index[:500], size=data.num_nodes)\n",
        "    data.test_mask = index_to_mask(rest_index[500:1500], size=data.num_nodes)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def random_coauthor_amazon_splits(data, num_classes, lcc_mask=None, seed=None):\n",
        "    # Set random coauthor/co-purchase splits:\n",
        "    # * 20 * num_classes labels for training\n",
        "    # * 30 * num_classes labels for validation\n",
        "    # rest labels for testing\n",
        "    g = None\n",
        "    if seed is not None:\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(seed)\n",
        "\n",
        "    indices = []\n",
        "    if lcc_mask is not None:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y[lcc_mask] == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0), generator=g)]\n",
        "            indices.append(index)\n",
        "    else:\n",
        "        for i in range(num_classes):\n",
        "            index = (data.y == i).nonzero().view(-1)\n",
        "            index = index[torch.randperm(index.size(0), generator=g)]\n",
        "            indices.append(index)\n",
        "\n",
        "    train_index = torch.cat([i[:20] for i in indices], dim=0)\n",
        "    val_index = torch.cat([i[20:50] for i in indices], dim=0)\n",
        "\n",
        "    rest_index = torch.cat([i[50:] for i in indices], dim=0)\n",
        "    rest_index = rest_index[torch.randperm(rest_index.size(0))]\n",
        "\n",
        "    data.train_mask = index_to_mask(train_index, size=data.num_nodes)\n",
        "    data.val_mask = index_to_mask(val_index, size=data.num_nodes)\n",
        "    data.test_mask = index_to_mask(rest_index, size=data.num_nodes)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vdO8owV9FoLK"
      },
      "outputs": [],
      "source": [
        "#model.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from typing import Optional, Tuple\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from torch import Tensor\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "\n",
        "class AirGNN(torch.nn.Module):\n",
        "    def __init__(self, dataset, args):\n",
        "        super(AirGNN, self).__init__()\n",
        "        self.dropout = args.dropout\n",
        "        self.lin1 = Linear(dataset.num_features, args.hidden)\n",
        "        self.lin2 = Linear(args.hidden, dataset.num_classes)\n",
        "        self.prop = AdaptiveMessagePassing(K=args.K, alpha=args.alpha, mode=args.model, args=args)\n",
        "        print(self.prop)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "        self.prop.reset_parameters()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, y = data.x, data.adj_t, data.y\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lin2(x)\n",
        "        x = self.prop(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class AdaptiveMessagePassing(MessagePassing):\n",
        "    _cached_edge_index: Optional[Tuple[Tensor, Tensor]]\n",
        "    _cached_adj_t: Optional[SparseTensor]\n",
        "\n",
        "    def __init__(self,\n",
        "                 K: int,\n",
        "                 alpha: float,\n",
        "                 dropout: float = 0.,\n",
        "                 cached: bool = False,\n",
        "                 add_self_loops: bool = True,\n",
        "                 normalize: bool = True,\n",
        "                 mode: str = None,\n",
        "                 node_num: int = None,\n",
        "                 args=None,\n",
        "                 **kwargs):\n",
        "\n",
        "        super(AdaptiveMessagePassing, self).__init__(aggr='add', **kwargs)\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "        self.mode = mode\n",
        "        self.dropout = dropout\n",
        "        self.cached = cached\n",
        "        self.add_self_loops = add_self_loops\n",
        "        self.normalize = normalize\n",
        "        self._cached_edge_index = None\n",
        "        self.node_num = node_num\n",
        "        self.args = args\n",
        "        self._cached_adj_t = None\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self._cached_edge_index = None\n",
        "        self._cached_adj_t = None\n",
        "\n",
        "    def forward(self, x: Tensor, edge_index: Adj, edge_weight: OptTensor = None, mode=None) -> Tensor:\n",
        "        if self.normalize:\n",
        "            if isinstance(edge_index, Tensor):\n",
        "                raise ValueError('Only support SparseTensor now')\n",
        "\n",
        "            elif isinstance(edge_index, SparseTensor):\n",
        "                cache = self._cached_adj_t\n",
        "                if cache is None:\n",
        "                    edge_index = gcn_norm(  # yapf: disable\n",
        "                        edge_index, edge_weight, x.size(self.node_dim), False,\n",
        "                        add_self_loops=self.add_self_loops, dtype=x.dtype)\n",
        "                    if self.cached:\n",
        "                        self._cached_adj_t = edge_index\n",
        "                else:\n",
        "                    edge_index = cache\n",
        "\n",
        "        if mode == None: mode = self.mode\n",
        "\n",
        "        if self.K <= 0:\n",
        "            return x\n",
        "        hh = x\n",
        "\n",
        "        if mode == 'MLP':\n",
        "            return x\n",
        "\n",
        "        elif mode == 'APPNP':\n",
        "            x = self.appnp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K, alpha=self.alpha)\n",
        "\n",
        "        elif mode in ['AirGNN']:\n",
        "            x = self.amp_forward(x=x, hh=hh, edge_index=edge_index, K=self.K)\n",
        "        else:\n",
        "            raise ValueError('wrong propagate mode')\n",
        "        return x\n",
        "\n",
        "    def appnp_forward(self, x, hh, edge_index, K, alpha):\n",
        "        for k in range(K):\n",
        "            x = self.propagate(edge_index, x=x, edge_weight=None, size=None)\n",
        "            x = x * (1 - alpha)\n",
        "            x += alpha * hh\n",
        "        return x\n",
        "\n",
        "\n",
        "    def compute_fast_KL(self, m1, s1, p1, m2, s2, p2):\n",
        "        '''Compute the KLDivergence between two gaussians, KL(P|Q) scaled between 0 and 1'''\n",
        "        #note: precision = p = inv(s)\n",
        "        d = len(m1)\n",
        "        kl = 0.5*( np.log2( np.linalg.det(s2)/np.linalg.det(s1) ) - d + np.trace(np.matmul(p2, s1)) + ((m2-m1).T)@(p2)@(m2-m1) )\n",
        "        scaled_kl = 1 - np.exp(-kl)\n",
        "\n",
        "        return scaled_kl\n",
        "\n",
        "    def amp_forward(self, x, hh, K, edge_index):\n",
        "        lambda_amp = self.args.lambda_amp\n",
        "        gamma = 1 / (2 * (1 - lambda_amp))  ## or simply gamma = 1\n",
        "        global XX\n",
        "        global YY\n",
        "        global HH\n",
        "        global gKL\n",
        "        HH = hh\n",
        "        XX = x\n",
        "        #print(f\"size of x = {x.size()},value of K = {K},size of hh = {hh.size()}\")\n",
        "        for k in range(K):\n",
        "            y = x - gamma * 2 * (1 - lambda_amp) * self.compute_LX(x=x, edge_index=edge_index)  # Equation (9)\n",
        "            if k==0:\n",
        "              YY = y\n",
        "              #print(f\"size of y = {y.size()}\")\n",
        "            # gmXin = GaussianMixture(n_components=1, random_state=0).fit(hh.detach().to('cpu').numpy())\n",
        "            # gmY = GaussianMixture(n_components=1, random_state=0).fit(y.detach().to('cpu').numpy())\n",
        "            # scaled_KL = self.compute_fast_KL(m1=gmXin.means_[0], s1=gmXin.covariances_[0], p1 = gmXin.precisions_[0],\n",
        "            #                                 m2=gmY.means_[0], s2 = gmY.covariances_[0], p2 = gmY.precisions_[0])\n",
        "            # gKL.append(scaled_KL)\n",
        "            gmX = [j for j in [GaussianMixture(n_components = 1,random_state = 0).fit(HH[i].detach().to('cpu').numpy().reshape(-1,1)) for i in range(0,2708)]]\n",
        "            gmy = [j for j in [GaussianMixture(n_components = 1,random_state = 0).fit(YY[i].detach().to('cpu').numpy().reshape(-1,1)) for i in range(0,2708)]]\n",
        "\n",
        "            kl = [k for k in [self.compute_fast_KL(m1 = gmX[i].means_[0],s1 = gmX[i].covariances_[0],p1 = gmX[i].precisions_[0],\n",
        "                                              m2 = gmy[i].means_[0],s2 = gmy[i].covariances_[0],p2 = gmy[i].precisions_[0]) for i in range(2708)]]\n",
        "            \n",
        "            # gmX = [j for j in GaussianMixture(n_components = 1,random_state = 0).fit(hh[i].detach().to('cpu').numpy()) for i in range(0,2708)]\n",
        "            # gmy = [j for j in GaussianMixture(n_components = 1,random_state = 0).fit(y[i].detach().to('cpu').numpy()) for i in range(0,2708)]\n",
        "            # kl = [k for k in ]\n",
        "            #x = hh + (1-scaled_KL)*(y-hh) #we use 1-kl because we want to more heavily weigh closer samples.\n",
        "            #x = hh + (scaled_KL)*(y-hh) #we use 1-kl because we want to more heavily weigh closer samples.\n",
        "            x = hh.to('cuda') + torch.tensor(kl).unsqueeze(1).to('cuda')*(y-hh).to('cuda')\n",
        "\n",
        "            #x = hh + self.proximal_L21(x=y - hh, lambda_=gamma * lambda_amp) # Equation (11) and (12)\n",
        "        return x\n",
        "\n",
        "    def proximal_L21(self, x: Tensor, lambda_):\n",
        "        row_norm = torch.norm(x, p=2, dim=1)\n",
        "        score = torch.clamp(row_norm - lambda_, min=0)\n",
        "        index = torch.where(row_norm > 0)             #  Deal with the case when the row_norm is 0\n",
        "        score[index] = score[index] / row_norm[index] # score is the adaptive score in Equation (14)\n",
        "        return score.unsqueeze(1) * x\n",
        "\n",
        "    def compute_LX(self, x, edge_index, edge_weight=None):\n",
        "        x = x - self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
        "        return x\n",
        "\n",
        "    def message(self, x_j: Tensor, edge_weight: Tensor) -> Tensor:\n",
        "        return edge_weight.view(-1, 1) * x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        return matmul(adj_t, x, reduce=self.aggr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(K={}, alpha={}, mode={}, dropout={}, lambda_amp={})'.format(self.__class__.__name__, self.K,\n",
        "                                                               self.alpha, self.mode, self.dropout,\n",
        "                                                               self.args.lambda_amp)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqyrNrcBG81q",
        "outputId": "a4dc5f44-346e-4710-eadd-4ec5679f1036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arg :  <__main__.Args object at 0x7f71b68a65d0>\n",
            "device: in train_eval: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "#from datasets import prepare_data\n",
        "#from model import AirGNN\n",
        "from tqdm import tqdm\n",
        "#from train_eval import evaluate\n",
        "import numpy as np\n",
        "from torch import tensor\n",
        "#from datasets import uniqueId, str2bool\n",
        "import torch_geometric.transforms as T\n",
        "from deeprobust.graph.data import Dpr2Pyg\n",
        "\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='Cora')\n",
        "# parser.add_argument('--model', type=str, default='AirGNN')\n",
        "# parser.add_argument('--normalize_features', type=str2bool, default=True)\n",
        "# parser.add_argument('--random_splits', type=str2bool, default=False)\n",
        "# parser.add_argument('--runs', type=int, default=10)\n",
        "# parser.add_argument('--dropout', type=float, default=0.5, help=\"dropout\")\n",
        "# parser.add_argument('--hidden', type=int, default=64)\n",
        "# parser.add_argument('--K', type=int, default=10, help=\"the number of propagagtion in AirGNN\")\n",
        "# parser.add_argument('--alpha', type=float, default=None)\n",
        "# parser.add_argument('--lambda_amp', type=float, default=0.1)\n",
        "\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = Args()\n",
        "\n",
        "print('arg : ', args)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"device: in train_eval:\", device)\n",
        "\n",
        "\n",
        "def main():\n",
        "    acc_lst_dic = {}\n",
        "    final_result = {}\n",
        "    n_perturbations_candidates = [0, 1, 2, 5, 10, 20, 50, 80]\n",
        "    for perturbation_number in n_perturbations_candidates:\n",
        "        acc_lst_dic[perturbation_number] = []\n",
        "\n",
        "    if args.dataset in [\"Cora\", \"CiteSeer\", \"PubMed\"]:\n",
        "        for run_k in range(args.runs):\n",
        "            perturbation_acc_dic = nettack_run(args.dataset.lower(), run_k, n_perturbations_candidates)\n",
        "            for key, val in perturbation_acc_dic.items():\n",
        "                acc_lst_dic[key].append(val)\n",
        "\n",
        "        for key, val in acc_lst_dic.items():\n",
        "            acc_lst = tensor(val)\n",
        "            final_result[key] = '{:.3f} ± {:.3f}'.format(acc_lst.mean().item(), acc_lst.std().item())\n",
        "            \n",
        "        print(\"Dataset:{}, model:{}\".format(args.dataset, args.model))\n",
        "        print(\"Average performance on 40 targeted nodes with 10 runs:\", final_result)\n",
        "\n",
        "def nettack_run(dataset_name, run_k, n_perturbations_candidates):\n",
        "    node_list = get_target_nodelst(dataset_name)\n",
        "    num = len(node_list)\n",
        "    assert num == 40\n",
        "\n",
        "    target_accuracy_dic = {}\n",
        "    target_accuracy_summary_dic = {}\n",
        "    for key in n_perturbations_candidates:\n",
        "        target_accuracy_dic[key] = 0\n",
        "        target_accuracy_summary_dic[key] = []\n",
        "\n",
        "    for target_node in node_list:\n",
        "        for perturbation in n_perturbations_candidates:\n",
        "            n_perturbations = int(perturbation)\n",
        "            uid = uniqueId(dataset_name, target_node, perturbation)\n",
        "            data = get_adv_data(uid)\n",
        "            target_node_acc = adv_test(target_node, data, data.adj[target_node].nonzero()[1].tolist(), run_k)\n",
        "            if target_node_acc == 0:\n",
        "                target_accuracy_dic[perturbation] += 1\n",
        "\n",
        "            print(\"=========Attacked Node: {:d}, n_perturbations: {:.2f}=========\".format(target_node, perturbation))\n",
        "        print(args.model, args.lambda_amp)\n",
        "\n",
        "    assert num == 40\n",
        "    for key in target_accuracy_dic.keys():\n",
        "        target_accuracy_dic[key] = 1 - target_accuracy_dic[key] / num\n",
        "\n",
        "    print(\"Accuracy on 40 target nodes:\", target_accuracy_dic)\n",
        "    return target_accuracy_dic\n",
        "\n",
        "\n",
        "def get_target_nodelst(dataset_name):\n",
        "    allfiles = os.listdir(\"./fixed_data/adv_attack\")\n",
        "    remain = [single_file for single_file in allfiles if dataset_name.lower() in single_file]\n",
        "    node_indexes = []\n",
        "    for i in remain:\n",
        "        if len(i.split(\"_\"))  == 3:\n",
        "            node_indexes.append(int(i.split(\"_\")[1]))\n",
        "\n",
        "    node_list = list(set(node_indexes))\n",
        "    assert len(node_list) == 40\n",
        "    return node_list\n",
        "\n",
        "def get_adv_data(uid):\n",
        "    print(\"./fixed_data/adv_attack/\"+uid+\".pickle\")\n",
        "    if os.path.isfile(\"./fixed_data/adv_attack/\"+uid+\".pickle\"):\n",
        "        return pickle.load(open(\"./fixed_data/adv_attack/\"+uid+\".pickle\",'rb'))\n",
        "    else:\n",
        "        raise Exception(\"ERROR\" + uid + \" file not found\")\n",
        "\n",
        "def adv_test(key_node_index, attacked_dpr_data, neighbor_lst, run_k):\n",
        "    transform = T.ToSparseTensor()\n",
        "    dataset = Dpr2Pyg(attacked_dpr_data, transform=transform)\n",
        "    data = dataset[0]\n",
        "    data = data.to(device)\n",
        "\n",
        "    if args.model in [\"APPNP\", \"AirGNN\", \"MLP\"]:\n",
        "        model = AirGNN(dataset, args)\n",
        "    else:\n",
        "        raise Exception(\"Unsupported model mode!!!\")\n",
        "\n",
        "    # 10 best models will be tested on the same attacked data\n",
        "    model.to(device).reset_parameters()\n",
        "    checkpointPath = \"./model/lcc/{}_{}_best_model_run_{}.pth\".format(args.dataset, args.model, run_k)\n",
        "    print(\"checkpointPath:\", checkpointPath)\n",
        "    checkpoint = torch.load(checkpointPath)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "    # test the prediction of targeted node\n",
        "    model.eval()\n",
        "    logits = model(data)\n",
        "    probs = torch.exp(logits[[key_node_index]])\n",
        "    target_node_acc = (logits.argmax(1)[key_node_index] == data.y[key_node_index]).item()  # True/False\n",
        "    # print(\"single_target_predict\\n:\", logits.argmax(1)[key_node_index].item(), data.y[key_node_index].item(), target_node_acc)\n",
        "    return target_node_acc\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "r5cu2jl8HrmQ"
      },
      "outputs": [],
      "source": [
        "#train_eval.py\n",
        "from __future__ import division\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import tensor\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "from torch_geometric.utils import *\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "\n",
        "def run(dataset, model, runs, epochs, lr, weight_decay, early_stopping,\n",
        "        permute_masks=None, logger=None, lcc=False, save_path=None, args=None, target_node=None):\n",
        "    val_losses, accs, durations = [], [], []\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"device: in train_eval:\", device)\n",
        "\n",
        "    data = dataset[0]\n",
        "    \n",
        "    pbar = tqdm(range(runs), unit='run')\n",
        "\n",
        "    for runs_num, _ in enumerate(pbar):\n",
        "        if permute_masks is not None:\n",
        "            data = permute_masks(data, dataset.num_classes, lcc_mask=None, seed=runs_num)\n",
        "        data = data.to(device)\n",
        "\n",
        "        model.to(device).reset_parameters()\n",
        "        optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "        t_start = time.perf_counter()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        test_acc = 0\n",
        "        val_loss_history = []\n",
        "        \n",
        "        if args.lcc:\n",
        "            path = (\"./model/lcc/{}_{}_best_model_run_{}.pth\".format(args.dataset, args.model, runs_num))\n",
        "        else:\n",
        "            path = (\"./model/full/{}_{}_best_model_run_{}.pth\".format(args.dataset, args.model, runs_num))\n",
        "        \n",
        "        for epoch in range(1, epochs + 1):\n",
        "            out = train(model, optimizer, data)\n",
        "            eval_info = evaluate(model, data)\n",
        "            eval_info['epoch'] = epoch\n",
        "\n",
        "            if logger is not None:\n",
        "                logger(eval_info)\n",
        "\n",
        "            if eval_info['val_loss'] < best_val_loss:\n",
        "                best_val_loss = eval_info['val_loss']\n",
        "                test_acc = eval_info['test_acc']\n",
        "\n",
        "                if args.model_cache:\n",
        "                    # print(\"*** Saving Checkpoint ***\")\n",
        "                    torch.save({\n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                        'optimizer_state_dict': optimizer.state_dict()\n",
        "                    }, path)\n",
        "\n",
        "            val_loss_history.append(eval_info['val_loss'])\n",
        "            if early_stopping > 0 and epoch > epochs // 2:\n",
        "                tmp = tensor(val_loss_history[-(early_stopping + 1):-1])\n",
        "                if eval_info['val_loss'] > tmp.mean().item():\n",
        "                    break\n",
        "\n",
        "        # to print results of this run\n",
        "        if logger is not None:\n",
        "            logger.print_statistics(runs_num)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        t_end = time.perf_counter()\n",
        "\n",
        "        val_losses.append(best_val_loss)\n",
        "        accs.append(test_acc)\n",
        "        durations.append(t_end - t_start)\n",
        "\n",
        "    # to print best results of all runs\n",
        "    if logger is not None:\n",
        "        logger.print_statistics()\n",
        "\n",
        "    loss, acc, duration = tensor(val_losses), tensor(accs), tensor(durations)\n",
        "\n",
        "    print('Val Loss: {:.4f}, Test Accuracy: {:.3f} ± {:.3f}, Duration: {:.3f}'.\n",
        "          format(loss.mean().item(),\n",
        "                 acc.mean().item(),\n",
        "                 acc.std().item(),\n",
        "                 duration.mean().item()))\n",
        "    return acc.mean().item()\n",
        "\n",
        "\n",
        "def train(model, optimizer, data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    if len(data.y.shape) == 1:\n",
        "        y = data.y\n",
        "    else:\n",
        "        y = data.y.squeeze(1) ## for ogb data\n",
        "\n",
        "    loss = F.nll_loss(out[data.train_mask], y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "    outs = {}\n",
        "\n",
        "    for key in ['train', 'val', 'test']:\n",
        "        mask = data['{}_mask'.format(key)]\n",
        "        # print(\"number:\", key, len(mask), mask.sum().item())\n",
        "        # print(key, mask)\n",
        "        if len(data.y.shape) == 1:\n",
        "            y = data.y\n",
        "        else:\n",
        "            y = data.y.squeeze(1) ## for ogb data\n",
        "\n",
        "        loss = F.nll_loss(logits[mask], y[mask]).item()\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(y[mask]).sum().item() / mask.sum().item()\n",
        "\n",
        "        outs['{}_loss'.format(key)] = loss\n",
        "        outs['{}_acc'.format(key)] = acc\n",
        "    return outs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIuOCAPyIAmz",
        "outputId": "25c2e321-4cb0-4348-8505-3990b25b5fad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arg :  <__main__.Args object at 0x7f710441ec90>\n",
            "Data: Data(x=[2708, 1433], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], adj_t=[2708, 2708, nnz=10556])\n",
            "AdaptiveMessagePassing(K=10, alpha=0.1, mode=AirGNN, dropout=0.0, lambda_amp=0.1)\n",
            "device: in train_eval: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [18:58<2:50:48, 1138.71s/run]"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import optuna\n",
        "import torch_geometric.transforms as T\n",
        "import argparse\n",
        "#from datasets import str2bool\n",
        "#from train_eval import run\n",
        "#from datasets import prepare_data\n",
        "#from model import AirGNN\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--dataset', type=str, default='Cora')\n",
        "# parser.add_argument('--seed', type=int, default=15)\n",
        "# parser.add_argument('--model', type=str, default='AirGNN')\n",
        "# parser.add_argument('--alpha', type=float, default=0.1)\n",
        "# parser.add_argument('--lambda_amp', type=float, default=0.1)\n",
        "# parser.add_argument('--lcc', type=str2bool, default=False)\n",
        "# parser.add_argument('--normalize_features', type=str2bool, default=True)\n",
        "# parser.add_argument('--random_splits', type=str2bool, default=False)\n",
        "# parser.add_argument('--runs', type=int, default=10)\n",
        "# parser.add_argument('--epochs', type=int, default=1000)\n",
        "# parser.add_argument('--lr', type=float, default=0.01)\n",
        "# parser.add_argument('--weight_decay', type=float, default=0.0005)\n",
        "# parser.add_argument('--early_stopping', type=int, default=100)\n",
        "# parser.add_argument('--hidden', type=int, default=64)\n",
        "# parser.add_argument('--dropout', type=float, default=0.8, help=\"dropout\")\n",
        "# parser.add_argument('--K', type=int, default=10, help=\"the number of propagagtion in AirGNN\")\n",
        "# parser.add_argument('--model_cache', type=str2bool, default=False)\n",
        "\n",
        "\n",
        "def main():\n",
        "    #args = parser.parse_args()\n",
        "    args = Args()\n",
        "    print('arg : ', args)\n",
        "    dataset, permute_masks = prepare_data(args, lcc=args.lcc)\n",
        "    model = AirGNN(dataset, args)\n",
        "    test_acc = run(dataset, model, args.runs, args.epochs, args.lr, args.weight_decay, args.early_stopping,\n",
        "                   permute_masks, logger=None, args=args) ## TODO: test or val acc\n",
        "    return test_acc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G-xq289A9k-"
      },
      "outputs": [],
      "source": [
        "len(gKL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxvDbViBBnHo"
      },
      "outputs": [],
      "source": [
        "plt.hist(gKL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrPv1D7xP-Ft"
      },
      "outputs": [],
      "source": [
        "gKL = []\n",
        "class Args:\n",
        "  dataset = 'Cora'\n",
        "  seed = 15\n",
        "  model = 'AirGNN'\n",
        "  alpha = 0.1\n",
        "  lambda_amp = 0.4\n",
        "  lcc = False\n",
        "  normalize_features = True\n",
        "  random_splits = False\n",
        "  runs = 1\n",
        "  epochs = 200\n",
        "  lr = 0.01\n",
        "  weight_decay = 0.0005\n",
        "  early_stopping = 100\n",
        "  hidden = 64\n",
        "  dropout = 0.8\n",
        "  K = 10\n",
        "  model_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5BG-YqiNiHP"
      },
      "outputs": [],
      "source": [
        "args = Args()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "F1VriFmQN02B",
        "outputId": "f2f106b3-447d-4850-e97d-55d085bbc38a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Cora'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args.dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "HoqF_-eJN1eM",
        "outputId": "5eed5812-5788-4435-afe7-a9dfbcbc1841"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-59371870f3cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'content' is not defined"
          ]
        }
      ],
      "source": [
        "/content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKWMZ1z86IkL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWHMRU5qZpyG"
      },
      "source": [
        "#KS Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOfcGf6rZr72"
      },
      "outputs": [],
      "source": [
        "XX = 10\n",
        "YY= 10\n",
        "HH = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akYrfs8zetOX",
        "outputId": "95151ca0-a729-4c0b-8955-588f47ee321b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2708, 7])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "XX.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_qOaWtDfoNF",
        "outputId": "fb475e0f-50a8-4d67-baba-3b0f622f0099"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 0.1240,  0.0994, -0.1357,  0.1353,  0.1677, -0.1541, -0.0091],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YY[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqYHIlrRfpnt",
        "outputId": "1d77feaf-0244-44ac-c02b-e8845f0f0da6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7, 1)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "HH[0].detach().to('cpu').numpy().reshape(-1,1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "XhKGAkOH6YIf",
        "outputId": "2a0fb867-6b82-4cbe-efd7-583a8278452d"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-d3dfb613dfbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgmy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mGaussianMixture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2708\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m kl = [k for k in [compute_fast_KL(m1 = gmX.means[0],s1 = gmX.covariances_[0],p1 = gmX.precisions_[0],\n\u001b[0m\u001b[1;32m      5\u001b[0m                                   m2 = gmy.means_[0],s2 = gmy.covariances_[0],p2 = gmy.precisions_[0])]]\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'means'"
          ]
        }
      ],
      "source": [
        "gmX = [j for j in [GaussianMixture(n_components = 1,random_state = 0).fit(HH[i].detach().to('cpu').numpy().reshape(-1,1)) for i in range(0,2708)]]\n",
        "gmy = [j for j in [GaussianMixture(n_components = 1,random_state = 0).fit(YY[i].detach().to('cpu').numpy().reshape(-1,1)) for i in range(0,2708)]]\n",
        "\n",
        "kl = [k for k in [compute_fast_KL(m1 = gmX.means_[0],s1 = gmX.covariances_[0],p1 = gmX.precisions_[0],\n",
        "                                  m2 = gmy.means_[0],s2 = gmy.covariances_[0],p2 = gmy.precisions_[0])]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO9xbyxC9JIn"
      },
      "outputs": [],
      "source": [
        "\n",
        "kl = [k for k in [compute_fast_KL(m1 = gmX[i].means_[0],s1 = gmX[i].covariances_[0],p1 = gmX[i].precisions_[0],\n",
        "                                  m2 = gmy[i].means_[0],s2 = gmy[i].covariances_[0],p2 = gmy[i].precisions_[0]) for i in range(2708)]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1-E0ygM9iFw",
        "outputId": "fe61b18a-c3fc-4533-e265-9c130080d916"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([-0.04392191, -0.0438972 , -0.04385958, ...,  0.74925885,\n",
              "         0.82448404,  0.86358212]), array([1, 1, 1, ..., 1, 1, 1]))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.unique(kl,return_counts=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mry1eEGyCrI9",
        "outputId": "fd480126-bb07-494b-8e7b-7d1a8c9a0c7b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2708,)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "qSnFDLFG_5yE",
        "outputId": "c6b6bedd-c5f4-48e0-e594-07f607c8b3fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([2.041e+03, 3.360e+02, 1.680e+02, 6.900e+01, 4.400e+01, 2.600e+01,\n",
              "        1.200e+01, 6.000e+00, 4.000e+00, 2.000e+00]),\n",
              " array([-0.04392191,  0.04682849,  0.1375789 ,  0.2283293 ,  0.3190797 ,\n",
              "         0.40983011,  0.50058051,  0.59133091,  0.68208132,  0.77283172,\n",
              "         0.86358212]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS9klEQVR4nO3dcaxe9X3f8fen0NCtCYsT3yJqOzOJTDWHbSa5IkxdOipaMHTCZJuoLTU4GYqTBqZGiTaR9g+iREh0LYmGlpE5xQKmBEJLMyyVjLqUFnWqEy6JazCEciFm2HPgNmTQjY4V+O6P59z4iXOv73Pv8/i5tn/vl/Tonud7fuec33Nkf+65v3Oec1JVSJLa8GPL3QFJ0vgY+pLUEENfkhpi6EtSQwx9SWrIqcvdgYWsXLmy1q5du9zdkKQTxsMPP/xXVTUx17zjPvTXrl3L1NTUcndDkk4YSZ6Zb57DO5LUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JDj/hu5w1h77R8sy3b33/BLy7JdSVrIgkf6SdYkeSDJY0n2Jfm1rv6WJLuSPNn9XNHVk+SmJNNJ9iZ5V9+6tnbtn0yy9dh9LEnSXAYZ3nkV+ERVrQfOB65Osh64Fri/qtYB93fvAS4B1nWvbcDN0PslAVwHvAc4D7hu9heFJGk8Fgz9qjpUVd/spv8aeBxYBWwCbuua3QZc3k1vAm6vnt3Am5OcCVwM7KqqF6rq+8AuYONIP40k6agWdSI3yVrgXODrwBlVdaib9V3gjG56FfBs32IHutp89bm2sy3JVJKpmZmZxXRRknQUA4d+kjcCdwMfq6qX+udVVQE1qk5V1faqmqyqyYmJOW8JLUlagoFCP8mP0wv8L1XV73fl57phG7qfz3f1g8CavsVXd7X56pKkMRnk6p0AtwCPV9Vn+2btBGavwNkK3NNXv7K7iud84MVuGOg+4KIkK7oTuBd1NUnSmAxynf7PAu8HHkmyp6v9OnADcFeSq4BngCu6efcClwLTwMvABwGq6oUknwEe6tp9uqpeGMmnkCQNZMHQr6o/AzLP7AvnaF/A1fOsawewYzEdlCSNjrdhkKSGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZJDHJe5I8nySR/tqX0myp3vtn32iVpK1Sf6mb94X+pZ5d5JHkkwnual7DKMkaYwGeVzircB/BG6fLVTVL89OJ7kReLGv/VNVtWGO9dwMfAj4Or1HKm4Evrb4LkuSlmrBI/2qehCY81m23dH6FcAdR1tHkjOB06tqd/c4xduByxffXUnSMIYd038v8FxVPdlXOyvJt5L8aZL3drVVwIG+Nge62pySbEsylWRqZmZmyC5KkmYNG/pb+OGj/EPA26rqXODjwJeTnL7YlVbV9qqarKrJiYmJIbsoSZo1yJj+nJKcCvwL4N2ztap6BXilm344yVPA2cBBYHXf4qu7miRpjIY50v8F4NtV9YNhmyQTSU7ppt8OrAOerqpDwEtJzu/OA1wJ3DPEtiVJSzDIJZt3AH8O/EySA0mu6mZt5kdP4P4csLe7hPP3gI9U1exJ4I8CvwNMA0/hlTuSNHYLDu9U1ZZ56h+Yo3Y3cPc87aeAcxbZP0nSCPmNXElqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwZ5ctaOJM8nebSv9qkkB5Ps6V6X9s37ZJLpJE8kubivvrGrTSe5dvQfRZK0kEGO9G8FNs5R/1xVbehe9wIkWU/vMYrv7Jb5T0lO6Z6b+3ngEmA9sKVrK0kao0Eel/hgkrUDrm8TcGdVvQJ8J8k0cF43b7qqngZIcmfX9rFF91iStGTDjOlfk2RvN/yzoqutAp7ta3Ogq81XlySN0VJD/2bgHcAG4BBw48h6BCTZlmQqydTMzMwoVy1JTVtS6FfVc1X1WlW9DnyRw0M4B4E1fU1Xd7X56vOtf3tVTVbV5MTExFK6KEmaw5JCP8mZfW/fB8xe2bMT2JzktCRnAeuAbwAPAeuSnJXkDfRO9u5cerclSUux4IncJHcAFwArkxwArgMuSLIBKGA/8GGAqtqX5C56J2hfBa6uqte69VwD3AecAuyoqn0j/zSSpKMa5OqdLXOUbzlK++uB6+eo3wvcu6jeSZJGym/kSlJDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMWDP0kO5I8n+TRvtpvJfl2kr1JvprkzV19bZK/SbKne32hb5l3J3kkyXSSm5Lk2HwkSdJ8BjnSvxXYeERtF3BOVf0j4C+BT/bNe6qqNnSvj/TVbwY+RO9h6evmWKck6RhbMPSr6kHghSNqf1hVr3ZvdwOrj7aOJGcCp1fV7qoq4Hbg8qV1WZK0VKMY0//XwNf63p+V5FtJ/jTJe7vaKuBAX5sDXW1OSbYlmUoyNTMzM4IuSpJgyNBP8hvAq8CXutIh4G1VdS7wceDLSU5f7HqrantVTVbV5MTExDBdlCT1OXWpCyb5APDPgQu7IRuq6hXglW764SRPAWcDB/nhIaDVXU2SNEZLOtJPshH4d8BlVfVyX30iySnd9NvpnbB9uqoOAS8lOb+7audK4J6hey9JWpQFj/ST3AFcAKxMcgC4jt7VOqcBu7orL3d3V+r8HPDpJH8LvA58pKpmTwJ/lN6VQH+H3jmA/vMAkqQxWDD0q2rLHOVb5ml7N3D3PPOmgHMW1TtJ0kj5jVxJaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMGCv0kO5I8n+TRvtpbkuxK8mT3c0VXT5Kbkkwn2ZvkXX3LbO3aP5lk6+g/jiTpaAY90r8V2HhE7Vrg/qpaB9zfvQe4hN6zcdcB24CbofdLgt6jFt8DnAdcN/uLQpI0HgOFflU9CLxwRHkTcFs3fRtweV/99urZDbw5yZnAxcCuqnqhqr4P7OJHf5FIko6hYcb0z6iqQ930d4EzuulVwLN97Q50tfnqPyLJtiRTSaZmZmaG6KIkqd9ITuRWVQE1inV169teVZNVNTkxMTGq1UpS84YJ/ee6YRu6n8939YPAmr52q7vafHVJ0pgME/o7gdkrcLYC9/TVr+yu4jkfeLEbBroPuCjJiu4E7kVdTZI0JqcO0ijJHcAFwMokB+hdhXMDcFeSq4BngCu65vcClwLTwMvABwGq6oUknwEe6tp9uqqOPDksSTqGBgr9qtoyz6wL52hbwNXzrGcHsGPg3kmSRspv5EpSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDlhz6SX4myZ6+10tJPpbkU0kO9tUv7Vvmk0mmkzyR5OLRfARJ0qAGelziXKrqCWADQJJTgIPAV+k9E/dzVfXb/e2TrAc2A+8Efhr4oyRnV9VrS+2DJGlxRjW8cyHwVFU9c5Q2m4A7q+qVqvoOvQennzei7UuSBjCq0N8M3NH3/poke5PsSLKiq60Cnu1rc6Cr/Ygk25JMJZmamZkZURclSUOHfpI3AJcBv9uVbgbeQW/o5xBw42LXWVXbq2qyqiYnJiaG7aIkqTOKI/1LgG9W1XMAVfVcVb1WVa8DX+TwEM5BYE3fcqu7miRpTEYR+lvoG9pJcmbfvPcBj3bTO4HNSU5LchawDvjGCLYvSRrQkq/eAUjyk8AvAh/uK//7JBuAAvbPzquqfUnuAh4DXgWu9sodSRqvoUK/qv4P8NYjau8/SvvrgeuH2aYkaen8Rq4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGjeDD6/iSPJNmTZKqrvSXJriRPdj9XdPUkuSnJdJK9Sd417PYlSYMb1ZH+z1fVhqqa7N5fC9xfVeuA+7v30HuI+rrutQ24eUTblyQN4FgN72wCbuumbwMu76vfXj27gTcf8SB1SdIxNIrQL+APkzycZFtXO6OqDnXT3wXO6KZXAc/2LXugq/2QJNuSTCWZmpmZGUEXJUkw5IPRO/+0qg4m+SlgV5Jv98+sqkpSi1lhVW0HtgNMTk4uallJ0vyGPtKvqoPdz+eBrwLnAc/NDtt0P5/vmh8E1vQtvrqrSZLGYKjQT/KTSd40Ow1cBDwK7AS2ds22Avd00zuBK7ureM4HXuwbBpIkHWPDDu+cAXw1yey6vlxV/y3JQ8BdSa4CngGu6NrfC1wKTAMvAx8ccvuSpEUYKvSr6mngH89R/x5w4Rz1Aq4eZpuSpKXzG7mS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDRnE/fR1h7bV/sGzb3n/DLy3btiUd/zzSl6SGGPqS1BBDX5IaYuhLUkOWHPpJ1iR5IMljSfYl+bWu/qkkB5Ps6V6X9i3zySTTSZ5IcvEoPoAkaXDDXL3zKvCJqvpm95zch5Ps6uZ9rqp+u79xkvXAZuCdwE8Df5Tk7Kp6bYg+SJIWYclH+lV1qKq+2U3/NfA4sOooi2wC7qyqV6rqO/Sek3veUrcvSVq8kYzpJ1kLnAt8vStdk2Rvkh1JVnS1VcCzfYsdYJ5fEkm2JZlKMjUzMzOKLkqSGEHoJ3kjcDfwsap6CbgZeAewATgE3LjYdVbV9qqarKrJiYmJYbsoSeoMFfpJfpxe4H+pqn4foKqeq6rXqup14IscHsI5CKzpW3x1V5MkjckwV+8EuAV4vKo+21c/s6/Z+4BHu+mdwOYkpyU5C1gHfGOp25ckLd4wV+/8LPB+4JEke7rarwNbkmwACtgPfBigqvYluQt4jN6VP1d75Y4kjdeSQ7+q/gzIHLPuPcoy1wPXL3WbkqTh+I1cSWqIoS9JDTH0JakhPkTlJLNcD3Dx4S3SicEjfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaojfyNVILNc3gcFvA0uL4ZG+JDXEI32d8LzfkDQ4j/QlqSFjP9JPshH4D8ApwO9U1Q3j7oM0Cv6FoRPRWI/0k5wCfB64BFhP73m668fZB0lq2biP9M8DpqvqaYAkdwKb6D0sXdIAlvNKqRadbH9ZjTv0VwHP9r0/ALznyEZJtgHburf/O8kTY+jbkVYCf7UM2z0euS8Oc18c1sS+yG8O1Ox42xd/f74Zx+XVO1W1Hdi+nH1IMlVVk8vZh+OF++Iw98Vh7ovDTqR9Me6rdw4Ca/rer+5qkqQxGHfoPwSsS3JWkjcAm4GdY+6DJDVrrMM7VfVqkmuA++hdsrmjqvaNsw+LsKzDS8cZ98Vh7ovD3BeHnTD7IlW13H2QJI2J38iVpIYY+pLUkKZDP8nGJE8kmU5y7RzzT0vylW7+15OsHX8vx2OAffHxJI8l2Zvk/iTzXgd8oltoX/S1+5dJKskJcaneUgyyL5Jc0f3b2Jfky+Pu4zgN8P/kbUkeSPKt7v/KpcvRz6OqqiZf9E4kPwW8HXgD8BfA+iPafBT4Qje9GfjKcvd7GffFzwN/t5v+1Zb3RdfuTcCDwG5gcrn7vYz/LtYB3wJWdO9/arn7vcz7Yzvwq930emD/cvf7yFfLR/o/uCVEVf0/YPaWEP02Abd1078HXJgkY+zjuCy4L6rqgap6uXu7m953LE5Gg/y7APgM8JvA/x1n58ZskH3xIeDzVfV9gKp6fsx9HKdB9kcBp3fTfw/4n2Ps30BaDv25bgmxar42VfUq8CLw1rH0brwG2Rf9rgK+dkx7tHwW3BdJ3gWsqaqT/SY4g/y7OBs4O8l/T7K7u4vuyWqQ/fEp4FeSHADuBf7NeLo2uOPyNgw6fiX5FWAS+GfL3ZflkOTHgM8CH1jmrhwvTqU3xHMBvb/+HkzyD6vqfy1rr5bPFuDWqroxyT8B/kuSc6rq9eXu2KyWj/QHuSXED9okOZXen2vfG0vvxmug22Mk+QXgN4DLquqVMfVt3BbaF28CzgH+JMl+4Hxg50l6MneQfxcHgJ1V9bdV9R3gL+n9EjgZDbI/rgLuAqiqPwd+gt7N2I4bLYf+ILeE2Als7ab/FfDH1Z2hOcksuC+SnAv8Z3qBfzKP2x51X1TVi1W1sqrWVtVaeuc3LquqqeXp7jE1yP+R/0rvKJ8kK+kN9zw9zk6O0SD7438AFwIk+Qf0Qn9mrL1cQLOh343Rz94S4nHgrqral+TTSS7rmt0CvDXJNPBxYN7L905kA+6L3wLeCPxukj1JTsp7Jg24L5ow4L64D/hekseAB4B/W1Un41/Dg+6PTwAfSvIXwB3AB463A0VvwyBJDWn2SF+SWmToS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIb8fygZdbO+ZFDBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(kl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzxGOXN77_V8",
        "outputId": "6b5da52d-d0ea-46d2-9df7-3693fcf8506d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2708,)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(gmy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzdTxcrO7_YU",
        "outputId": "23d3c9a6-ed39-42d7-d9a2-296b0b19aa42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.03499064062322888"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gmX[0].means_[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn7casvq6xlF"
      },
      "outputs": [],
      "source": [
        "ks_scores_statistic = [i for i,j in [kstest(HH[a],YY[a]) for a in range(2708)]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxwN5RZNfqwo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_fast_KL( m1, s1, p1, m2, s2, p2):\n",
        "    '''Compute the KLDivergence between two gaussians, KL(P|Q) scaled between 0 and 1'''\n",
        "    #note: precision = p = inv(s)\n",
        "    d = len(m1)\n",
        "    #print(d)\n",
        "    kl = 0.5*( np.log2( np.linalg.det(s2)/np.linalg.det(s1) ) - d + np.trace(np.matmul(p2, s1)) + ((m2-m1).T)@(p2)@(m2-m1) )\n",
        "    scaled_kl = 1 - np.exp(-kl)\n",
        "\n",
        "    return scaled_kl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVb1lIhUigRG",
        "outputId": "0840221d-9010-43c9-ee33-4356ace897e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "gmXin = GaussianMixture(n_components=1, random_state=0).fit(HH.detach().to('cpu').numpy())\n",
        "gmY = GaussianMixture(n_components=1, random_state=0).fit(YY.detach().to('cpu').numpy())\n",
        "scaled_KL = compute_fast_KL(m1=gmXin.means_[0], s1=gmXin.covariances_[0], p1 = gmXin.precisions_[0],\n",
        "            m2=gmY.means_[0], s2 = gmY.covariances_[0], p2 = gmY.precisions_[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VNzGwEny31N",
        "outputId": "c86a4058-ce1d-43cc-b7da-456d10ac3df8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.14681828,  0.105377  , -0.13598958,  0.0999996 ,  0.14964377,\n",
              "       -0.13393594, -0.01462713])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gmXin.means_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me-FIjGbipNt",
        "outputId": "c52d4602-c4a0-49d0-8c70-2fdf2836171c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9738049663821569"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scaled_KL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-W8zoTRCjIfL"
      },
      "outputs": [],
      "source": [
        "\n",
        "sscore = 10\n",
        "bscore = 10\n",
        "def proximal_L21(x: Tensor, lambda_):\n",
        "    print(x.size())\n",
        "    row_norm = torch.norm(x, p=2, dim=1)\n",
        "    #print(row_norm)\n",
        "    print(f\"row norm size = {row_norm.size()}\")\n",
        "    global bscore\n",
        "    score = torch.clamp(row_norm - lambda_, min=0)\n",
        "    bscore = score\n",
        "    print(f\"score size = {score.size()}\")\n",
        "\n",
        "    index = torch.where(row_norm > 0)             #  Deal with the case when the row_norm is 0\n",
        "    print(f\"row index size = {index[0].size()}\")\n",
        "    #print\n",
        "    score[index] = score[index] / row_norm[index] # score is the adaptive score in Equation (14)\n",
        "    global sscore\n",
        "    sscore = score\n",
        "    print(f\" size = {row_norm.size()}\")\n",
        "    print(score.unsqueeze(1).size())\n",
        "    return score.unsqueeze(1) * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQrAdt3tqdZU"
      },
      "outputs": [],
      "source": [
        "\n",
        "gamma = 1 / (2 * (1 - args.lambda_amp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hGq2rjgqHkP",
        "outputId": "07f08b74-be58-49a6-c1f9-dbd22060451c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2708, 7])\n",
            "row norm size = torch.Size([2708])\n",
            "score size = torch.Size([2708])\n",
            "row index size = torch.Size([2708])\n",
            " size = torch.Size([2708])\n",
            "torch.Size([2708, 1])\n",
            "torch.Size([2708, 7])\n",
            "row norm size = torch.Size([2708])\n",
            "score size = torch.Size([2708])\n",
            "row index size = torch.Size([2708])\n",
            " size = torch.Size([2708])\n",
            "torch.Size([2708, 1])\n"
          ]
        }
      ],
      "source": [
        "p = proximal_L21(x = YY-HH,lambda_ = gamma*args.lambda_amp)\n",
        "x = HH + proximal_L21(x=YY - HH, lambda_=gamma * args.lambda_amp) # Equation (11) and (12)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSKX6Jw4mWdh"
      },
      "outputs": [],
      "source": [
        "p.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzW6Qc9HBqeK"
      },
      "outputs": [],
      "source": [
        "np.unique(bscore,return_counts = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9Y0M-QOBsgK"
      },
      "source": [
        "# Proximal using KS Test statistic\n",
        "* Compute ks test for each row of of yy with each row of hh to get score and then multiply score with x "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCY2FPDWTyjO"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import kstest\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fuu0s-eT2Y5"
      },
      "outputs": [],
      "source": [
        "np.shape(HH[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uL4bW9mT2fR"
      },
      "outputs": [],
      "source": [
        "YY[2].size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DXL-4rJUNHY"
      },
      "outputs": [],
      "source": [
        "kstest(YY[1000].detach().numpy().flatten(),YY[5].detach().numpy().flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJu9vgjGbL5_"
      },
      "outputs": [],
      "source": [
        "YY[2707]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9i0czE8bPNj"
      },
      "outputs": [],
      "source": [
        "YY[55]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anDK4kXWUNM5"
      },
      "outputs": [],
      "source": [
        "np.unique(ks_scores,return_counts = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkPIpR3tl4XQ"
      },
      "outputs": [],
      "source": [
        "np.unique(ks_scores_statistic,return_counts = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtQJFrcxUNR6"
      },
      "outputs": [],
      "source": [
        "ks_scores = [j for i,j in [kstest(HH[a],YY[a]) for a in range(2708)]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlnn0oEhlvN4"
      },
      "outputs": [],
      "source": [
        "ks_scores_statistic = [i for i,j in [kstest(HH[a],YY[a]) for a in range(2708)]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73LIWXtTl1GZ"
      },
      "outputs": [],
      "source": [
        "ind = np.where()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxMGcNpmdOkJ"
      },
      "outputs": [],
      "source": [
        "yd = YY[1000].detach().numpy().flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_arsGtTdf73"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.sort(yd),1.*np.arange(7)/6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_udoAa19fvzf"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.sort(yd),1.*np.arange(7)/6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hNQC6Oyj7qD"
      },
      "outputs": [],
      "source": [
        "np.unique(ks_scores,return_counts= 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9x1Dqi4kQdk"
      },
      "outputs": [],
      "source": [
        "np.unique(sscore,return_counts = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcXJf80wqPdI"
      },
      "outputs": [],
      "source": [
        "gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X23_yE2YqRy7"
      },
      "outputs": [],
      "source": [
        "x.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuq1fc8esD_g"
      },
      "outputs": [],
      "source": [
        "sscore.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExNyfa5xsH59"
      },
      "outputs": [],
      "source": [
        "x[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XCX_0Bwszkc"
      },
      "outputs": [],
      "source": [
        "XX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUkcUPlNs_ez"
      },
      "outputs": [],
      "source": [
        "sscore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMqd9a8ntrwR"
      },
      "outputs": [],
      "source": [
        "sscore.unsqueeze(1).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmctMzT9t20d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyOmJrxfAyEUz94F+h/hKFsB",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}